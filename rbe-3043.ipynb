{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":3.475241,"end_time":"2024-12-10T17:57:14.479626","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-10T17:57:11.004385","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"8316349b","cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# For example, here's several helpful packages to load\n\n\n\nimport numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n\n    for filename in filenames:\n\n        print(os.path.join(dirname, filename))\n\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-12-10T20:17:11.975765Z","iopub.execute_input":"2024-12-10T20:17:11.976083Z","iopub.status.idle":"2024-12-10T20:17:12.322211Z","shell.execute_reply.started":"2024-12-10T20:17:11.976054Z","shell.execute_reply":"2024-12-10T20:17:12.321499Z"},"papermill":{"duration":0.712062,"end_time":"2024-12-10T17:57:14.160146","exception":false,"start_time":"2024-12-10T17:57:13.448084","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":2},{"id":"d195bfd1-a390-4435-bdff-e1fe23f0ff39","cell_type":"code","source":"%pip install PyOpenGL\n\n%pip install pyvirtualdisplay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:16:51.428952Z","iopub.execute_input":"2024-12-10T20:16:51.429298Z","iopub.status.idle":"2024-12-10T20:17:11.974018Z","shell.execute_reply.started":"2024-12-10T20:16:51.429268Z","shell.execute_reply":"2024-12-10T20:17:11.972868Z"}},"outputs":[{"name":"stdout","text":"Collecting PyOpenGL\n  Downloading PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\nDownloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: PyOpenGL\nSuccessfully installed PyOpenGL-3.1.7\nNote: you may need to restart the kernel to use updated packages.\nCollecting pyvirtualdisplay\n  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\nDownloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\nInstalling collected packages: pyvirtualdisplay\nSuccessfully installed pyvirtualdisplay-3.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"id":"328dfe3f-d185-41fe-bd24-31ef613e45cc","cell_type":"code","source":"!pip install gym==0.25.2 \n!pip install gymnasium==1.0.0 \n!pip install imutils==0.5.4 \n!pip install Jinja2==3.1.4 \n!pip install joblib \n!pip install libclang==18.1.1 \n!pip install Markdown==3.7 \n!pip install MarkupSafe==3.0.2 \n!pip install matplotlib==3.9.3 \n!pip install panda-gym==3.0.7 \n!pip install pillow==11.0.0 \n!pip install pybullet==3.2.6 \n!pip install six==1.16.0 \n!pip install sympy==1.13.1 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:17:12.323539Z","iopub.execute_input":"2024-12-10T20:17:12.323854Z","iopub.status.idle":"2024-12-10T20:19:52.380534Z","shell.execute_reply.started":"2024-12-10T20:17:12.323830Z","shell.execute_reply":"2024-12-10T20:19:52.379362Z"}},"outputs":[{"name":"stdout","text":"Collecting gym==0.25.2\n  Downloading gym-0.25.2.tar.gz (734 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.5/734.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.2) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.2) (3.1.0)\nRequirement already satisfied: gym_notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym==0.25.2) (0.0.8)\nBuilding wheels for collected packages: gym\n  Building wheel for gym (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for gym: filename=gym-0.25.2-py3-none-any.whl size=852299 sha256=bf2c1f5d4efbb4ebd38c40d1ce91c14ea31cf09a15ed8d87a05fd345cae0a992\n  Stored in directory: /root/.cache/pip/wheels/78/95/2c/ee47a8d43fda6a851e340e77e27cf75b49ff4ce2d1540c0e80\nSuccessfully built gym\nInstalling collected packages: gym\n  Attempting uninstall: gym\n    Found existing installation: gym 0.26.2\n    Uninstalling gym-0.26.2:\n      Successfully uninstalled gym-0.26.2\nSuccessfully installed gym-0.25.2\nCollecting gymnasium==1.0.0\n  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0) (3.1.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium==1.0.0) (0.0.4)\nDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: gymnasium\n  Attempting uninstall: gymnasium\n    Found existing installation: gymnasium 0.29.0\n    Uninstalling gymnasium-0.29.0:\n      Successfully uninstalled gymnasium-0.29.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.16.9 requires gymnasium==0.29.0, but you have gymnasium 1.0.0 which is incompatible.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed gymnasium-1.0.0\nCollecting imutils==0.5.4\n  Downloading imutils-0.5.4.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: imutils\n  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25837 sha256=c03081d05cf318f751c3a9283b6b994a7306f246b000a3bfea752aa7cac06164\n  Stored in directory: /root/.cache/pip/wheels/85/cf/3a/e265e975a1e7c7e54eb3692d6aa4e2e7d6a3945d29da46f2d7\nSuccessfully built imutils\nInstalling collected packages: imutils\nSuccessfully installed imutils-0.5.4\nRequirement already satisfied: Jinja2==3.1.4 in /opt/conda/lib/python3.10/site-packages (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2==3.1.4) (2.1.5)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (1.4.2)\nRequirement already satisfied: libclang==18.1.1 in /opt/conda/lib/python3.10/site-packages (18.1.1)\nCollecting Markdown==3.7\n  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\nDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: Markdown\n  Attempting uninstall: Markdown\n    Found existing installation: Markdown 3.6\n    Uninstalling Markdown-3.6:\n      Successfully uninstalled Markdown-3.6\nSuccessfully installed Markdown-3.7\nCollecting MarkupSafe==3.0.2\n  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nDownloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\nInstalling collected packages: MarkupSafe\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.1.5\n    Uninstalling MarkupSafe-2.1.5:\n      Successfully uninstalled MarkupSafe-2.1.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab 4.3.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nydata-profiling 4.12.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed MarkupSafe-3.0.2\nCollecting matplotlib==3.9.3\n  Downloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (1.4.5)\nRequirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (21.3)\nRequirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.9.3) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.9.3) (1.16.0)\nDownloading matplotlib-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: matplotlib\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n      Successfully uninstalled matplotlib-3.7.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbeatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.3.1 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\nydata-profiling 4.12.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed matplotlib-3.9.3\nCollecting panda-gym==3.0.7\n  Downloading panda_gym-3.0.7-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: gymnasium>=0.26 in /opt/conda/lib/python3.10/site-packages (from panda-gym==3.0.7) (1.0.0)\nCollecting pybullet (from panda-gym==3.0.7)\n  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from panda-gym==3.0.7) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from panda-gym==3.0.7) (1.14.1)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.26->panda-gym==3.0.7) (3.1.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.26->panda-gym==3.0.7) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.26->panda-gym==3.0.7) (0.0.4)\nDownloading panda_gym-3.0.7-py3-none-any.whl (23 kB)\nDownloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pybullet, panda-gym\nSuccessfully installed panda-gym-3.0.7 pybullet-3.2.6\nCollecting pillow==11.0.0\n  Downloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\nDownloading pillow-11.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pillow\n  Attempting uninstall: pillow\n    Found existing installation: pillow 10.3.0\n    Uninstalling pillow-10.3.0:\n      Successfully uninstalled pillow-10.3.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.0.0 which is incompatible.\nydata-profiling 4.12.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pillow-11.0.0\nRequirement already satisfied: pybullet==3.2.6 in /opt/conda/lib/python3.10/site-packages (3.2.6)\nRequirement already satisfied: six==1.16.0 in /opt/conda/lib/python3.10/site-packages (1.16.0)\nCollecting sympy==1.13.1\n  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy==1.13.1) (1.3.0)\nDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sympy\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.3\n    Uninstalling sympy-1.13.3:\n      Successfully uninstalled sympy-1.13.3\nSuccessfully installed sympy-1.13.1\n","output_type":"stream"}],"execution_count":3},{"id":"2493a91c-1853-4eb7-87b6-9ffd400af578","cell_type":"code","source":"from pyvirtualdisplay import Display\n\ndisplay = Display(visible=0, size=(1024, 768))\ndisplay.start()\n\nfrom matplotlib import pyplot as plt, animation\n%matplotlib inline\nfrom IPython import display\n\ndef create_anim(frames, dpi, fps):\n\n    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n\n    patch = plt.imshow(frames[0])\n\n    def setup():\n\n        plt.axis('off')\n\n    def animate(i):\n\n        patch.set_data(frames[i])\n\n    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n\n    return anim\n\n\ndef display_anim(frames, dpi=72, fps=50):\n\n    anim = create_anim(frames, dpi, fps)\n\n    return anim.to_jshtml()\n\n\ndef save_anim(frames, filename, dpi=72, fps=50):\n\n    anim = create_anim(frames, dpi, fps)\n\n    anim.save(filename)\n\n\nclass trigger:\n\n    def __init__(self):\n\n        self._trigger = True\n\n    def __call__(self, e):\n\n        return self._trigger\n\n    def set(self, t):\n\n        self._trigger = t","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:20:08.575853Z","iopub.execute_input":"2024-12-10T20:20:08.576829Z","iopub.status.idle":"2024-12-10T20:20:08.666650Z","shell.execute_reply.started":"2024-12-10T20:20:08.576644Z","shell.execute_reply":"2024-12-10T20:20:08.665938Z"}},"outputs":[],"execution_count":6},{"id":"f1272ba3-937d-40b0-8911-25dd6d9c7f8e","cell_type":"code","source":"!git clone https://github.com/HoangTungMai/RL.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:19:52.986897Z","iopub.execute_input":"2024-12-10T20:19:52.987160Z","iopub.status.idle":"2024-12-10T20:19:54.593934Z","shell.execute_reply.started":"2024-12-10T20:19:52.987134Z","shell.execute_reply":"2024-12-10T20:19:54.593073Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'RL'...\nremote: Enumerating objects: 28, done.\u001b[K\nremote: Counting objects: 100% (28/28), done.\u001b[K\nremote: Compressing objects: 100% (23/23), done.\u001b[K\nremote: Total 28 (delta 9), reused 13 (delta 3), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (28/28), 24.69 KiB | 6.17 MiB/s, done.\nResolving deltas: 100% (9/9), done.\n","output_type":"stream"}],"execution_count":5},{"id":"405670f7-f20d-447b-8b27-c600fd6abf4a","cell_type":"code","source":"import os\nos.chdir('../RBE3043-23-main/src')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:26:15.322700Z","iopub.execute_input":"2024-12-10T20:26:15.323489Z","iopub.status.idle":"2024-12-10T20:26:15.327238Z","shell.execute_reply.started":"2024-12-10T20:26:15.323453Z","shell.execute_reply":"2024-12-10T20:26:15.326380Z"}},"outputs":[],"execution_count":22},{"id":"a30b6d04-1b30-4c91-a063-6b714b8562b6","cell_type":"code","source":"from test_env import *\nfrom model import DiscreteActor, ContinuousActor, Critic\nimport torch\nfrom os import path\nfrom pathlib import Path\nfrom myTrain import Trainer\n\nMOVE = 0\nPICK = 1\nPLACE = 2\n\naction_space = {\n    'discrete': {'Move': 0, 'Pick': 1, 'Place': 2},\n    'continuous': [4, 4, 4]\n}\n\ndiscrete_dim = len(action_space['discrete'])\ncontinuous_dim = action_space['continuous']\n\nenv = My_Arm_RobotEnv(\n    observation_type=0,\n    render_mode='human',\n    blocker_bar=True,\n    objects_count=1,\n    sorting_count=1\n)\n\nobs, _ = env.reset()\nobs_dim = len(obs['observation'])\nd_actor = DiscreteActor(obs_dim=obs_dim, output_dim=discrete_dim)\nc_actor = ContinuousActor(obs_dim=obs_dim,\n                          continuous_param_dim=continuous_dim)\ncritic = Critic(obs_dim=obs_dim)\n\ntrainer = Trainer(\n    env=env,\n    discrete_actor=d_actor,\n    continuous_actor=c_actor,\n    critic=critic,\n    timesteps=2_000_000,\n    timesteps_per_batch=5_000,\n    max_timesteps_per_episode=750,\n)\n\nPath(\"./training\").mkdir(parents=True, exist_ok=True)\nif path.isfile(\"./training/state.data\"):\n    trainer.load(\"./training\")\ntrainer.train()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T20:26:22.503075Z","iopub.execute_input":"2024-12-10T20:26:22.503424Z"}},"outputs":[{"name":"stderr","text":"pybullet build time: Nov 28 2023 23:45:17\n/kaggle/working/RL/RBE3043-23-main/src/myTrain.py:119: RankWarning: Polyfit may be poorly conditioned\n  coefficients = np.polyfit(\n\n            =========================================\n            Timesteps: 750 / 2,000,000 (0.0375%)\n            Episodes: 1\n            Currently: Rollout\n            Latest Reward: -399\n            Latest Avg Rewards: -399\n            Recent Change: -199.43\n            Best Reward: -398.87\n            Latest Q_loss: 0.0\n            Latest X_loss: 0.0\n            Avg Q_loss: 0.0\n            Avg X_loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 1,500 / 2,000,000 (0.075%)\n            Episodes: 2\n            Currently: Rollout\n            Latest Reward: -380\n            Latest Avg Rewards: -389\n            Recent Change: -18.81\n            Best Reward: -380.06\n            Latest Q_loss: 0.0\n            Latest X_loss: 0.0\n            Avg Q_loss: 0.0\n            Avg X_loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 2,250 / 2,000,000 (0.1125%)\n            Episodes: 3\n            Currently: Rollout\n            Latest Reward: -380\n            Latest Avg Rewards: -386\n            Recent Change: -9.53\n            Best Reward: -379.8\n            Latest Q_loss: 0.0\n            Latest X_loss: 0.0\n            Avg Q_loss: 0.0\n            Avg X_loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 3,000 / 2,000,000 (0.15%)\n            Episodes: 4\n            Currently: Rollout\n            Latest Reward: -319\n            Latest Avg Rewards: -370\n            Recent Change: -23.86\n            Best Reward: -319.44\n            Latest Q_loss: 0.0\n            Latest X_loss: 0.0\n            Avg Q_loss: 0.0\n            Avg X_loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 3,750 / 2,000,000 (0.1875%)\n            Episodes: 5\n            Currently: Rollout\n            Latest Reward: -382\n            Latest Avg Rewards: -372\n            Recent Change: -9.42\n            Best Reward: -319.44\n            Latest Q_loss: 0.0\n            Latest X_loss: 0.0\n            Avg Q_loss: 0.0\n            Avg X_loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 4,500 / 2,000,000 (0.225%)\n            Episodes: 6\n            Currently: Rollout\n            Latest Reward: -363\n            Latest Avg Rewards: -371\n            Recent Change: -6.66\n            Best Reward: -319.44\n            Latest Q_loss: 0.0\n            Latest X_loss: 0.0\n            Avg Q_loss: 0.0\n            Avg X_loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,250 / 2,000,000 (0.2625%)\n            Episodes: 7\n            Currently: Rollout\n            Latest Reward: -409\n            Latest Avg Rewards: -376\n            Recent Change: -0.08\n            Best Reward: -319.44\n            Latest Q_loss: 0.0\n            Latest X_loss: 0.0\n            Avg Q_loss: 0.0\n            Avg X_loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,250 / 2,000,000 (0.2625%)\n            Episodes: 7\n            Currently: Training cycle 1/5\n            Latest Reward: -409\n            Latest Avg Rewards: -376\n            Recent Change: -0.08\n            Best Reward: -319.44\n            Latest Q_loss: 0.0\n            Latest X_loss: 0.0\n            Avg Q_loss: 0.0\n            Avg X_loss: 0.0\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,250 / 2,000,000 (0.2625%)\n            Episodes: 7\n            Currently: Training cycle 2/5\n            Latest Reward: -409\n            Latest Avg Rewards: -376\n            Recent Change: -0.08\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,250 / 2,000,000 (0.2625%)\n            Episodes: 7\n            Currently: Training cycle 3/5\n            Latest Reward: -409\n            Latest Avg Rewards: -376\n            Recent Change: -0.08\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,250 / 2,000,000 (0.2625%)\n            Episodes: 7\n            Currently: Training cycle 4/5\n            Latest Reward: -409\n            Latest Avg Rewards: -376\n            Recent Change: -0.08\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 5,250 / 2,000,000 (0.2625%)\n            Episodes: 7\n            Currently: Training cycle 5/5\n            Latest Reward: -409\n            Latest Avg Rewards: -376\n            Recent Change: -0.08\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 6,000 / 2,000,000 (0.3%)\n            Episodes: 8\n            Currently: Rollout\n            Latest Reward: -362\n            Latest Avg Rewards: -374\n            Recent Change: -1.22\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 6,750 / 2,000,000 (0.3375%)\n            Episodes: 9\n            Currently: Rollout\n            Latest Reward: -326\n            Latest Avg Rewards: -369\n            Recent Change: -4.04\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 7,500 / 2,000,000 (0.375%)\n            Episodes: 10\n            Currently: Rollout\n            Latest Reward: -367\n            Latest Avg Rewards: -369\n            Recent Change: -3.03\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 8,250 / 2,000,000 (0.4125%)\n            Episodes: 11\n            Currently: Rollout\n            Latest Reward: -394\n            Latest Avg Rewards: -371\n            Recent Change: -1.15\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 9,000 / 2,000,000 (0.45%)\n            Episodes: 12\n            Currently: Rollout\n            Latest Reward: -372\n            Latest Avg Rewards: -371\n            Recent Change: -0.86\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 9,750 / 2,000,000 (0.4875%)\n            Episodes: 13\n            Currently: Rollout\n            Latest Reward: -387\n            Latest Avg Rewards: -372\n            Recent Change: -0.14\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 10,500 / 2,000,000 (0.525%)\n            Episodes: 14\n            Currently: Rollout\n            Latest Reward: -378\n            Latest Avg Rewards: -373\n            Recent Change: 0.05\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 10,500 / 2,000,000 (0.525%)\n            Episodes: 14\n            Currently: Training cycle 1/5\n            Latest Reward: -378\n            Latest Avg Rewards: -373\n            Recent Change: 0.05\n            Best Reward: -319.44\n            Latest Q_loss: 2255.8411\n            Latest X_loss: 0.0518\n            Avg Q_loss: 2255.8411\n            Avg X_loss: 0.0518\n            =========================================\n        \n\n            =========================================\n            Timesteps: 10,500 / 2,000,000 (0.525%)\n            Episodes: 14\n            Currently: Training cycle 2/5\n            Latest Reward: -378\n            Latest Avg Rewards: -373\n            Recent Change: 0.05\n            Best Reward: -319.44\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2244.8068\n            Avg X_loss: 0.0516\n            =========================================\n        \n\n            =========================================\n            Timesteps: 10,500 / 2,000,000 (0.525%)\n            Episodes: 14\n            Currently: Training cycle 3/5\n            Latest Reward: -378\n            Latest Avg Rewards: -373\n            Recent Change: 0.05\n            Best Reward: -319.44\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2236.9253\n            Avg X_loss: 0.0514\n            =========================================\n        \n\n            =========================================\n            Timesteps: 10,500 / 2,000,000 (0.525%)\n            Episodes: 14\n            Currently: Training cycle 4/5\n            Latest Reward: -378\n            Latest Avg Rewards: -373\n            Recent Change: 0.05\n            Best Reward: -319.44\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2231.0141\n            Avg X_loss: 0.0513\n            =========================================\n        \n\n            =========================================\n            Timesteps: 10,500 / 2,000,000 (0.525%)\n            Episodes: 14\n            Currently: Training cycle 5/5\n            Latest Reward: -378\n            Latest Avg Rewards: -373\n            Recent Change: 0.05\n            Best Reward: -319.44\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2226.4165\n            Avg X_loss: 0.0512\n            =========================================\n        \n\n            =========================================\n            Timesteps: 11,250 / 2,000,000 (0.5625%)\n            Episodes: 15\n            Currently: Rollout\n            Latest Reward: -336\n            Latest Avg Rewards: -370\n            Recent Change: -0.88\n            Best Reward: -319.44\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2222.7384\n            Avg X_loss: 0.0511\n            =========================================\n        \n\n            =========================================\n            Timesteps: 12,000 / 2,000,000 (0.6%)\n            Episodes: 16\n            Currently: Rollout\n            Latest Reward: -345\n            Latest Avg Rewards: -369\n            Recent Change: -1.28\n            Best Reward: -319.44\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2222.7384\n            Avg X_loss: 0.0511\n            =========================================\n        \n\n            =========================================\n            Timesteps: 12,750 / 2,000,000 (0.6375%)\n            Episodes: 17\n            Currently: Rollout\n            Latest Reward: -375\n            Latest Avg Rewards: -369\n            Recent Change: -0.94\n            Best Reward: -319.44\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2222.7384\n            Avg X_loss: 0.0511\n            =========================================\n        \n\n            =========================================\n            Timesteps: 13,500 / 2,000,000 (0.675%)\n            Episodes: 18\n            Currently: Rollout\n            Latest Reward: -312\n            Latest Avg Rewards: -366\n            Recent Change: -1.79\n            Best Reward: -312.27\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2222.7384\n            Avg X_loss: 0.0511\n            =========================================\n        \n\n            =========================================\n            Timesteps: 14,250 / 2,000,000 (0.7125%)\n            Episodes: 19\n            Currently: Rollout\n            Latest Reward: -385\n            Latest Avg Rewards: -367\n            Recent Change: -1.21\n            Best Reward: -312.27\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2222.7384\n            Avg X_loss: 0.0511\n            =========================================\n        \n\n            =========================================\n            Timesteps: 15,000 / 2,000,000 (0.75%)\n            Episodes: 20\n            Currently: Rollout\n            Latest Reward: -358\n            Latest Avg Rewards: -366\n            Recent Change: -1.17\n            Best Reward: -312.27\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2222.7384\n            Avg X_loss: 0.0511\n            =========================================\n        \n\n            =========================================\n            Timesteps: 15,750 / 2,000,000 (0.7875%)\n            Episodes: 21\n            Currently: Rollout\n            Latest Reward: -330\n            Latest Avg Rewards: -365\n            Recent Change: -1.48\n            Best Reward: -312.27\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2222.7384\n            Avg X_loss: 0.0511\n            =========================================\n        \n\n            =========================================\n            Timesteps: 15,750 / 2,000,000 (0.7875%)\n            Episodes: 21\n            Currently: Training cycle 1/5\n            Latest Reward: -330\n            Latest Avg Rewards: -365\n            Recent Change: -1.48\n            Best Reward: -312.27\n            Latest Q_loss: 2189.6357\n            Latest X_loss: 0.0504\n            Avg Q_loss: 2222.7384\n            Avg X_loss: 0.0511\n            =========================================\n        \n\n            =========================================\n            Timesteps: 15,750 / 2,000,000 (0.7875%)\n            Episodes: 21\n            Currently: Training cycle 2/5\n            Latest Reward: -330\n            Latest Avg Rewards: -365\n            Recent Change: -1.48\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2198.1562\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 15,750 / 2,000,000 (0.7875%)\n            Episodes: 21\n            Currently: Training cycle 3/5\n            Latest Reward: -330\n            Latest Avg Rewards: -365\n            Recent Change: -1.48\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2177.671\n            Avg X_loss: 0.0509\n            =========================================\n        \n\n            =========================================\n            Timesteps: 15,750 / 2,000,000 (0.7875%)\n            Episodes: 21\n            Currently: Training cycle 4/5\n            Latest Reward: -330\n            Latest Avg Rewards: -365\n            Recent Change: -1.48\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2160.3374\n            Avg X_loss: 0.0509\n            =========================================\n        \n\n            =========================================\n            Timesteps: 15,750 / 2,000,000 (0.7875%)\n            Episodes: 21\n            Currently: Training cycle 5/5\n            Latest Reward: -330\n            Latest Avg Rewards: -365\n            Recent Change: -1.48\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2145.48\n            Avg X_loss: 0.0508\n            =========================================\n        \n\n            =========================================\n            Timesteps: 16,500 / 2,000,000 (0.825%)\n            Episodes: 22\n            Currently: Rollout\n            Latest Reward: -315\n            Latest Avg Rewards: -362\n            Recent Change: -1.88\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2132.6036\n            Avg X_loss: 0.0508\n            =========================================\n        \n\n            =========================================\n            Timesteps: 17,250 / 2,000,000 (0.8625%)\n            Episodes: 23\n            Currently: Rollout\n            Latest Reward: -369\n            Latest Avg Rewards: -363\n            Recent Change: -1.58\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2132.6036\n            Avg X_loss: 0.0508\n            =========================================\n        \n\n            =========================================\n            Timesteps: 18,000 / 2,000,000 (0.9%)\n            Episodes: 24\n            Currently: Rollout\n            Latest Reward: -347\n            Latest Avg Rewards: -362\n            Recent Change: -1.54\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2132.6036\n            Avg X_loss: 0.0508\n            =========================================\n        \n\n            =========================================\n            Timesteps: 18,750 / 2,000,000 (0.9375%)\n            Episodes: 25\n            Currently: Rollout\n            Latest Reward: -389\n            Latest Avg Rewards: -363\n            Recent Change: -1.12\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2132.6036\n            Avg X_loss: 0.0508\n            =========================================\n        \n\n            =========================================\n            Timesteps: 19,500 / 2,000,000 (0.975%)\n            Episodes: 26\n            Currently: Rollout\n            Latest Reward: -392\n            Latest Avg Rewards: -364\n            Recent Change: -0.75\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2132.6036\n            Avg X_loss: 0.0508\n            =========================================\n        \n\n            =========================================\n            Timesteps: 20,250 / 2,000,000 (1.0125%)\n            Episodes: 27\n            Currently: Rollout\n            Latest Reward: -380\n            Latest Avg Rewards: -365\n            Recent Change: -0.54\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2132.6036\n            Avg X_loss: 0.0508\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,000 / 2,000,000 (1.05%)\n            Episodes: 28\n            Currently: Rollout\n            Latest Reward: -319\n            Latest Avg Rewards: -363\n            Recent Change: -0.83\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2132.6036\n            Avg X_loss: 0.0508\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,000 / 2,000,000 (1.05%)\n            Episodes: 28\n            Currently: Training cycle 1/5\n            Latest Reward: -319\n            Latest Avg Rewards: -363\n            Recent Change: -0.83\n            Best Reward: -312.27\n            Latest Q_loss: 1952.3341\n            Latest X_loss: 0.0502\n            Avg Q_loss: 2132.6036\n            Avg X_loss: 0.0508\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,000 / 2,000,000 (1.05%)\n            Episodes: 28\n            Currently: Training cycle 2/5\n            Latest Reward: -319\n            Latest Avg Rewards: -363\n            Recent Change: -0.83\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2129.5124\n            Avg X_loss: 0.0508\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,000 / 2,000,000 (1.05%)\n            Episodes: 28\n            Currently: Training cycle 3/5\n            Latest Reward: -319\n            Latest Avg Rewards: -363\n            Recent Change: -0.83\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2126.7849\n            Avg X_loss: 0.0509\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,000 / 2,000,000 (1.05%)\n            Episodes: 28\n            Currently: Training cycle 4/5\n            Latest Reward: -319\n            Latest Avg Rewards: -363\n            Recent Change: -0.83\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2124.3604\n            Avg X_loss: 0.0509\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,000 / 2,000,000 (1.05%)\n            Episodes: 28\n            Currently: Training cycle 5/5\n            Latest Reward: -319\n            Latest Avg Rewards: -363\n            Recent Change: -0.83\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2122.1911\n            Avg X_loss: 0.0509\n            =========================================\n        \n\n            =========================================\n            Timesteps: 21,750 / 2,000,000 (1.0875%)\n            Episodes: 29\n            Currently: Rollout\n            Latest Reward: -381\n            Latest Avg Rewards: -364\n            Recent Change: -0.63\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2120.2388\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 22,500 / 2,000,000 (1.125%)\n            Episodes: 30\n            Currently: Rollout\n            Latest Reward: -368\n            Latest Avg Rewards: -364\n            Recent Change: -0.53\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2120.2388\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 23,250 / 2,000,000 (1.1625%)\n            Episodes: 31\n            Currently: Rollout\n            Latest Reward: -390\n            Latest Avg Rewards: -365\n            Recent Change: -0.33\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2120.2388\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 24,000 / 2,000,000 (1.2%)\n            Episodes: 32\n            Currently: Rollout\n            Latest Reward: -356\n            Latest Avg Rewards: -365\n            Recent Change: -0.35\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2120.2388\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 24,750 / 2,000,000 (1.2375%)\n            Episodes: 33\n            Currently: Rollout\n            Latest Reward: -402\n            Latest Avg Rewards: -366\n            Recent Change: -0.12\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2120.2388\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 25,500 / 2,000,000 (1.275%)\n            Episodes: 34\n            Currently: Rollout\n            Latest Reward: -327\n            Latest Avg Rewards: -365\n            Recent Change: -0.3\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2120.2388\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 26,250 / 2,000,000 (1.3125%)\n            Episodes: 35\n            Currently: Rollout\n            Latest Reward: -364\n            Latest Avg Rewards: -365\n            Recent Change: -0.28\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2120.2388\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 26,250 / 2,000,000 (1.3125%)\n            Episodes: 35\n            Currently: Training cycle 1/5\n            Latest Reward: -364\n            Latest Avg Rewards: -365\n            Recent Change: -0.28\n            Best Reward: -312.27\n            Latest Q_loss: 2083.1443\n            Latest X_loss: 0.0515\n            Avg Q_loss: 2120.2388\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 26,250 / 2,000,000 (1.3125%)\n            Episodes: 35\n            Currently: Training cycle 2/5\n            Latest Reward: -364\n            Latest Avg Rewards: -365\n            Recent Change: -0.28\n            Best Reward: -312.27\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2123.8922\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 26,250 / 2,000,000 (1.3125%)\n            Episodes: 35\n            Currently: Training cycle 3/5\n            Latest Reward: -364\n            Latest Avg Rewards: -365\n            Recent Change: -0.28\n            Best Reward: -312.27\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2127.2135\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 26,250 / 2,000,000 (1.3125%)\n            Episodes: 35\n            Currently: Training cycle 4/5\n            Latest Reward: -364\n            Latest Avg Rewards: -365\n            Recent Change: -0.28\n            Best Reward: -312.27\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2130.246\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 26,250 / 2,000,000 (1.3125%)\n            Episodes: 35\n            Currently: Training cycle 5/5\n            Latest Reward: -364\n            Latest Avg Rewards: -365\n            Recent Change: -0.28\n            Best Reward: -312.27\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2133.0257\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 27,000 / 2,000,000 (1.35%)\n            Episodes: 36\n            Currently: Rollout\n            Latest Reward: -346\n            Latest Avg Rewards: -364\n            Recent Change: -0.34\n            Best Reward: -312.27\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2135.5831\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 27,750 / 2,000,000 (1.3875%)\n            Episodes: 37\n            Currently: Rollout\n            Latest Reward: -309\n            Latest Avg Rewards: -363\n            Recent Change: -0.55\n            Best Reward: -308.66\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2135.5831\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 28,500 / 2,000,000 (1.425%)\n            Episodes: 38\n            Currently: Rollout\n            Latest Reward: -388\n            Latest Avg Rewards: -363\n            Recent Change: -0.4\n            Best Reward: -308.66\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2135.5831\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 29,250 / 2,000,000 (1.4625%)\n            Episodes: 39\n            Currently: Rollout\n            Latest Reward: -351\n            Latest Avg Rewards: -363\n            Recent Change: -0.42\n            Best Reward: -308.66\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2135.5831\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 30,000 / 2,000,000 (1.5%)\n            Episodes: 40\n            Currently: Rollout\n            Latest Reward: -386\n            Latest Avg Rewards: -363\n            Recent Change: -0.3\n            Best Reward: -308.66\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2135.5831\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 30,750 / 2,000,000 (1.5375%)\n            Episodes: 41\n            Currently: Rollout\n            Latest Reward: -371\n            Latest Avg Rewards: -364\n            Recent Change: -0.26\n            Best Reward: -308.66\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2135.5831\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 31,500 / 2,000,000 (1.575%)\n            Episodes: 42\n            Currently: Rollout\n            Latest Reward: -344\n            Latest Avg Rewards: -363\n            Recent Change: -0.3\n            Best Reward: -308.66\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2135.5831\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 31,500 / 2,000,000 (1.575%)\n            Episodes: 42\n            Currently: Training cycle 1/5\n            Latest Reward: -344\n            Latest Avg Rewards: -363\n            Recent Change: -0.3\n            Best Reward: -308.66\n            Latest Q_loss: 2196.9604\n            Latest X_loss: 0.051\n            Avg Q_loss: 2135.5831\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 31,500 / 2,000,000 (1.575%)\n            Episodes: 42\n            Currently: Training cycle 2/5\n            Latest Reward: -344\n            Latest Avg Rewards: -363\n            Recent Change: -0.3\n            Best Reward: -308.66\n            Latest Q_loss: 2018.7911\n            Latest X_loss: 0.0512\n            Avg Q_loss: 2131.0911\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 31,500 / 2,000,000 (1.575%)\n            Episodes: 42\n            Currently: Training cycle 3/5\n            Latest Reward: -344\n            Latest Avg Rewards: -363\n            Recent Change: -0.3\n            Best Reward: -308.66\n            Latest Q_loss: 2018.7911\n            Latest X_loss: 0.0512\n            Avg Q_loss: 2126.9319\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 31,500 / 2,000,000 (1.575%)\n            Episodes: 42\n            Currently: Training cycle 4/5\n            Latest Reward: -344\n            Latest Avg Rewards: -363\n            Recent Change: -0.3\n            Best Reward: -308.66\n            Latest Q_loss: 2018.7911\n            Latest X_loss: 0.0512\n            Avg Q_loss: 2123.0697\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 31,500 / 2,000,000 (1.575%)\n            Episodes: 42\n            Currently: Training cycle 5/5\n            Latest Reward: -344\n            Latest Avg Rewards: -363\n            Recent Change: -0.3\n            Best Reward: -308.66\n            Latest Q_loss: 2018.7911\n            Latest X_loss: 0.0512\n            Avg Q_loss: 2119.4739\n            Avg X_loss: 0.051\n            =========================================\n        \n\n            =========================================\n            Timesteps: 32,250 / 2,000,000 (1.6125%)\n            Episodes: 43\n            Currently: Rollout\n            Latest Reward: -356\n            Latest Avg Rewards: -363\n            Recent Change: -0.3\n            Best Reward: -308.66\n            Latest Q_loss: 2018.7911\n            Latest X_loss: 0.0512\n            Avg Q_loss: 2116.1178\n            Avg X_loss: 0.051\n            =========================================\n        \n","output_type":"stream"}],"execution_count":null}]}